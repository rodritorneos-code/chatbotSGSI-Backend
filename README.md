# üñ• Backend ‚Äì Chatbot SGSI

Backend del **Chatbot SGSI** orientado a **Sistema de Gesti√≥n de Seguridad de la Informaci√≥n (SGSI)**.  
Desarrollado en **Python** con **FastAPI**, soporte para **GPU (CUDA 12.8)** y ejecuci√≥n opcional en **CPU**.  
Permite exponer la API para consumo del frontend mediante un t√∫nel seguro con **Cloudflared**.

---

# ‚öô Instalaci√≥n

## 1Ô∏è‚É£ Crear entorno virtual
Se recomienda un entorno aislado para gestionar dependencias:

### Crear entorno virtual llamado gpu_env311
python -m venv gpu_env311

### Permitir ejecuci√≥n de scripts temporales
Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass

### Activar el entorno virtual
.\gpu_env311\Scripts\activate

### Actualizar pip
python -m pip install --upgrade pip

## 2Ô∏è‚É£ Instalar dependencias principales
### Framework web y servidor ASGI
pip install fastapi uvicorn requests

### Librer√≠as de audio
pip install soundfile sounddevice

### Librer√≠as para procesamiento de texto japon√©s (ejemplo de uso)
pip install mecab-python3 unidic-lite
python -m unidic download

### Text-to-Speech opcional
pip install git+https://github.com/myshell-ai/MeloTTS.git

### üí° Nota: Ajusta las dependencias seg√∫n tus necesidades de procesamiento o TTS.

## 3Ô∏è‚É£ Instalar llama-cpp con soporte CUDA
Descarga la versi√≥n precompilada de llama-cpp con soporte CUDA 12.8:
https://github.com/boneylizard/llama-cpp-python-cu128-gemma3/releases

### Instalar la versi√≥n descargada
pip install --force-reinstall C:\Users\Usuario\Downloads\llama_cpp_python-0.3.8+cu128.gemma3-cp311-cp311-win_amd64.whl

### Verificar instalaci√≥n
python -c "from llama_cpp import Llama; print('llama_cpp loaded successfully!')"

### Ejecutar prueba de GPU
python test_gpu.py

## Alternativa
### üí° Modo CPU:
pip install llama-cpp-python

## üöÄ Ejecutar Backend

### Ejecuta la API en modo desarrollo con recarga autom√°tica
uvicorn backend.main:app --reload

### Servidor local: http://127.0.0.1:8000

### Puedes usar Postman, curl o el frontend para probar la API.

## üåç Exponer backend con Cloudflared

### Para que el frontend desplegado en Vercel pueda comunicarse con tu backend local:

### Instalar Cloudflared
choco install cloudflared -y --force

### Crear t√∫nel p√∫blico a tu backend local
cloudflared tunnel --url http://127.0.0.1:8000

Esto genera una URL p√∫blica temporal.

Configura esta URL en el frontend desplegado para consumir la API.

‚ö†Ô∏è La URL cambia cada vez que reinicias el t√∫nel.

## üß† Modelos IA

Opcional usando Hugging Face:

### Instalar librer√≠as Hugging Face
pip install "huggingface_hub==0.19.4" "transformers==4.37.0"

### Autenticarse
hf auth login

### Descargar modelo Qwen2.5-7B en formato GGUF
hf download bartowski/Qwen2.5-7B-Instruct-GGUF Qwen2.5-7B-Instruct.gguf --local-dir ./Qwen2.5-7B-Instruct

### Tambi√©n puedes usar modelos Q4 o Q5
hf download bartowski/Qwen2.5-14B-Instruct-GGUF Qwen2.5-14B-Instruct-Q4_K_M.gguf --local-dir ./Qwen2.5-14B-Q4_K_M
hf download bartowski/Qwen2.5-14B-Instruct-GGUF Qwen2.5-14B-Instruct-Q5_K_M.gguf --local-dir ./Qwen2.5-14B-Instruct

## üìà Caracter√≠sticas T√©cnicas

### Arquitectura backend separada del frontend
### Soporte GPU (CUDA) y CPU
### Integraci√≥n de modelos LLM locales
### API REST para comunicaci√≥n con frontend
### Exposici√≥n p√∫blica mediante Cloudflared
### Escalable y modular

## üîß Verificaci√≥n de Sistema
### Verificar GPU NVIDIA
nvidia-smi

### Verificar versi√≥n CUDA
nvcc --version

## üìú Licencia
C√≥digo bajo licencia MIT

## üë®‚Äçüíª Autor
Rodrigo Alexander Pinto Ni√±o
